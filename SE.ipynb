{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_hfn1rwBao-F",
        "96A2mL-FaAYN",
        "gYItvvJ8agQq",
        "dgj9kwj_bXXX",
        "2BbNkJxYaHtO",
        "nqTvrQa0aPRc",
        "k0_woJfLaTDv",
        "zAYjpcBfab3-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wujFO0Fc0DE2",
        "colab_type": "code",
        "outputId": "06baea8e-6b7f-44ca-ea37-4cdcab1b6349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "gdrive_root = '/gdrive/My Drive'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfytM1T_1ItZ",
        "colab_type": "code",
        "outputId": "b55383d4-73eb-410c-b480-d0d29f27447c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "import os, csv\n",
        "import sys\n",
        "import json\n",
        "import logging\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "import torchvision \n",
        "from torchvision import transforms\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import tqdm\n",
        "\n",
        "!pip install -U tensorboardcolab\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "\n",
        "import scipy.signal\n",
        "from scipy.io import wavfile\n",
        "!pip install librosa\n",
        "import librosa\n",
        "\n",
        "!pip install https://github.com/vBaiCai/python-pesq/archive/master.zip\n",
        "!pip install pystoi\n",
        "!pip install soundfile\n",
        "import soundfile as sf\n",
        "from pystoi.stoi import stoi\n",
        "from pypesq import pesq\n",
        "\n",
        "!pip install GoogleDriveDownloader\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.8)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.17.4)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.14.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.40.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.21.3)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.1)\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.30.0)\n",
            "Collecting https://github.com/vBaiCai/python-pesq/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/vBaiCai/python-pesq/archive/master.zip\n",
            "\u001b[K     / 204kB 775kB/s\n",
            "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): pypesq==1.0 from https://github.com/vBaiCai/python-pesq/archive/master.zip in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pypesq==1.0) (1.17.4)\n",
            "Building wheels for collected packages: pypesq\n",
            "  Building wheel for pypesq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypesq: filename=pypesq-1.0-cp36-cp36m-linux_x86_64.whl size=80150 sha256=505ca63e7bcd6aae0a845d41a6d831331e16befe7c340458b61f21a11806515e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6djqhn0e/wheels/7b/1c/45/917e2bebf14f475e220194afeebb731d74215e10698e89a620\n",
            "Successfully built pypesq\n",
            "Requirement already satisfied: pystoi in /usr/local/lib/python3.6/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pystoi) (1.17.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pystoi) (1.3.2)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.6/dist-packages (0.10.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.13.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.19)\n",
            "Requirement already satisfied: GoogleDriveDownloader in /usr/local/lib/python3.6/dist-packages (0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hfn1rwBao-F",
        "colab_type": "text"
      },
      "source": [
        "# Training tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96A2mL-FaAYN",
        "colab_type": "text"
      },
      "source": [
        "## Data process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q0i8KOlMFv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download Train set\n",
        "gdd.download_file_from_google_drive(file_id='1CULVCAq0T3wqZTPGIqPja6OtwjYJkGAy',\n",
        "                                    dest_path='dataset/train.tar.gz',\n",
        "                                    unzip=False, showsize=True)\n",
        "get_ipython().system('tar xvzf dataset/train.tar.gz')\n",
        "# In[53]:\n",
        "\n",
        "\n",
        "# Download Valid set\n",
        "gdd.download_file_from_google_drive(file_id='1WE229Jt9WV2iZbxY7YjkYIfSZyCHz9Iq',\n",
        "                                    dest_path='dataset/valid.tar.gz',\n",
        "                                    unzip=False, showsize=True)\n",
        "get_ipython().system('tar xvzf dataset/valid.tar.gz')\n",
        "# In[ ]:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYItvvJ8agQq",
        "colab_type": "text"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbU7CzwUKmIx",
        "colab_type": "code",
        "outputId": "9395fb42-cf0b-4038-f018-b27180efde36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "def load_data_list(folder='/home/sangwon/바탕화면/dataset', setname='train'):\n",
        "    assert(setname in ['train', 'valid'])\n",
        "\n",
        "    dataset = {}\n",
        "    # foldername = folder + '/' + setname + 'set'\n",
        "    foldername = folder + '/' + setname\n",
        "    # print(foldername) # /home/sangwon/바탕화면/dataset/valid\n",
        "    \n",
        "\n",
        "    print(\"Loading files...\")\n",
        "    dataset['innames'] = []\n",
        "    dataset['outnames'] = []\n",
        "    dataset['shortnames'] = []\n",
        "\n",
        "    noisy_filelist = os.listdir(\"%s_noisy\"%(foldername))\n",
        "    # filelist = [f for f in filelist if f.endswith(\".wav\")]\n",
        "    for i in tqdm(noisy_filelist):\n",
        "        dataset['innames'].append(\"%s_noisy/%s\"%(foldername,i))\n",
        "        dataset['shortnames'].append(\"%s\"%(i))\n",
        "        \n",
        "    clean_filelist = os.listdir(\"%s_clean\"%(foldername))\n",
        "    for i in tqdm(clean_filelist):\n",
        "        dataset['outnames'].append(\"%s_clean/%s\"%(foldername,i))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# DATA LOADING - LOAD FILE DATA\n",
        "def load_data(dataset):\n",
        "\n",
        "    dataset['inaudio']  = [None]*len(dataset['innames'])\n",
        "    dataset['outaudio'] = [None]*len(dataset['outnames'])\n",
        "\n",
        "    for id in tqdm(range(len(dataset['innames']))):\n",
        "\n",
        "        if dataset['inaudio'][id] is None:\n",
        "            inputData, sr = librosa.load(dataset['innames'][id], sr=None)\n",
        "            outputData, sr = librosa.load(dataset['outnames'][id], sr=None)\n",
        "\n",
        "            shape = np.shape(inputData)\n",
        "\n",
        "            dataset['inaudio'][id]  = np.float32(inputData)\n",
        "            dataset['outaudio'][id] = np.float32(outputData)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "class AudioDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Audio sample reader.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_type):\n",
        "        dataset = load_data_list(setname=data_type)\n",
        "        self.dataset = load_data(dataset)\n",
        "\n",
        "        self.file_names = dataset['innames']\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mixed = torch.from_numpy(self.dataset['inaudio'][idx]).type(torch.FloatTensor)\n",
        "        clean = torch.from_numpy(self.dataset['outaudio'][idx]).type(torch.FloatTensor)\n",
        "\n",
        "        return mixed, clean\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def zero_pad_concat(self, inputs):\n",
        "        max_t = max(inp.shape[0] for inp in inputs)\n",
        "        shape = (len(inputs), max_t)\n",
        "        input_mat = np.zeros(shape, dtype=np.float32)\n",
        "        for e, inp in enumerate(inputs):\n",
        "            input_mat[e, :inp.shape[0]] = inp\n",
        "        return input_mat\n",
        "\n",
        "    def collate(self, inputs):\n",
        "        mixeds, cleans = zip(*inputs)\n",
        "        seq_lens = torch.IntTensor([i.shape[0] for i in mixeds])\n",
        "\n",
        "        x = torch.FloatTensor(self.zero_pad_concat(mixeds))\n",
        "        y = torch.FloatTensor(self.zero_pad_concat(cleans))\n",
        "\n",
        "        batch = [x, y, seq_lens]\n",
        "        return batch\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "# train_dataset = AudioDataset(data_type='train')\n",
        "# train_data_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size,\n",
        "#         collate_fn=train_dataset.collate, shuffle=True, num_workers=4)\n",
        "valid_dataset = AudioDataset(data_type='valid')\n",
        "\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "valid_data_loader = DataLoader(dataset=test_dataset, batch_size=4,\n",
        "        collate_fn=test_dataset.collate, shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "# In[6]:\n",
        "\n",
        "\n",
        "for tdl in iter(valid_data_loader):\n",
        "    print(tdl)\n",
        "    print(len(tdl))\n",
        "    print(tdl[0].size())\n",
        "    print(tdl[1].size())\n",
        "    print(tdl[2].size())\n",
        "    break\n",
        "\n",
        "\n",
        "# In[ ]:"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading files...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-513f947896ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# train_data_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m#         collate_fn=train_dataset.collate, shuffle=True, num_workers=4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mvalid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-513f947896ef>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_type)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-513f947896ef>\u001b[0m in \u001b[0;36mload_data_list\u001b[0;34m(folder, setname)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shortnames'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mnoisy_filelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s_noisy\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfoldername\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# filelist = [f for f in filelist if f.endswith(\".wav\")]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_filelist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/sangwon/바탕화면/dataset/valid_noisy'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgj9kwj_bXXX",
        "colab_type": "text"
      },
      "source": [
        "## Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BbNkJxYaHtO",
        "colab_type": "text"
      },
      "source": [
        "### complexnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJI5G_KE75aZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility functions for initialization\n",
        "def complex_rayleigh_init(Wr, Wi, fanin=None, gain=1):\n",
        "    if not fanin:\n",
        "        fanin = 1\n",
        "        for p in W1.shape[1:]: fanin *= p\n",
        "    scale  = float(gain) / float(fanin)\n",
        "    theta  = torch.empty_like(Wr).uniform_(-math.pi/2, +math.pi/2)\n",
        "    rho    = np.random.rayleigh(scale, tuple(Wr.shape))\n",
        "    rho    = torch.tensor(rho).to(Wr)\n",
        "    Wr.data.copy_(rho * theta.cos())\n",
        "    Wi.data.copy_(rho * theta.sin())\n",
        "\n",
        "# Layers\n",
        "class ComplexConvWrapper(nn.Module):\n",
        "    def __init__(self, conv_module, *args, **kwargs):\n",
        "        super(ComplexConvWrapper, self).__init__()\n",
        "        self.conv_re = conv_module(*args, **kwargs)\n",
        "        self.conv_im = conv_module(*args, **kwargs)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        fanin = self.conv_re.in_channels // self.conv_re.groups\n",
        "        for s in self.conv_re.kernel_size: fanin *= s\n",
        "        complex_rayleigh_init(self.conv_re.weight, self.conv_im.weight, fanin)\n",
        "        if self.conv_re.bias is not None:\n",
        "            self.conv_re.bias.data.zero_()\n",
        "            self.conv_im.bias.data.zero_()\n",
        "\n",
        "    def forward(self, xr, xi):\n",
        "        real = self.conv_re(xr) - self.conv_im(xi)\n",
        "        imag = self.conv_re(xi) + self.conv_im(xr)\n",
        "        return real, imag\n",
        "\n",
        "# Real-valued network module for complex input\n",
        "class RealConvWrapper(nn.Module):\n",
        "    def __init__(self, conv_module, *args, **kwargs):\n",
        "        super(ComplexConvWrapper,self).__init__()\n",
        "        self.conv_re = conv_module(*args, **kwargs)\n",
        "\n",
        "    def forward(self, xr, xi):\n",
        "        real = self.conv_re(xr)\n",
        "        imag = self.conv_re(xi)\n",
        "        return real, imag\n",
        "\n",
        "class CLeakyReLU(nn.LeakyReLU):\n",
        "    def forward(self, xr, xi):\n",
        "        return F.leaky_relu(xr, self.negative_slope, self.inplace),\\\n",
        "                F.leaky_relu(xi, self.negative_slope, self.inplace)\n",
        "\n",
        "# Source: https://github.com/ChihebTrabelsi/deep_complex_networks/tree/pytorch\n",
        "class ComplexBatchNorm(torch.nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n",
        "            track_running_stats=True):\n",
        "        super(ComplexBatchNorm, self).__init__()\n",
        "        self.num_features        = num_features\n",
        "        self.eps                 = eps\n",
        "        self.momentum            = momentum\n",
        "        self.affine              = affine\n",
        "        self.track_running_stats = track_running_stats\n",
        "        if self.affine:\n",
        "            self.Wrr = torch.nn.Parameter(torch.Tensor(num_features))\n",
        "            self.Wri = torch.nn.Parameter(torch.Tensor(num_features))\n",
        "            self.Wii = torch.nn.Parameter(torch.Tensor(num_features))\n",
        "            self.Br  = torch.nn.Parameter(torch.Tensor(num_features))\n",
        "            self.Bi  = torch.nn.Parameter(torch.Tensor(num_features))\n",
        "        else:\n",
        "            self.register_parameter('Wrr', None)\n",
        "            self.register_parameter('Wri', None)\n",
        "            self.register_parameter('Wii', None)\n",
        "            self.register_parameter('Br',  None)\n",
        "            self.register_parameter('Bi',  None)\n",
        "        if self.track_running_stats:\n",
        "            self.register_buffer('RMr',  torch.zeros(num_features))\n",
        "            self.register_buffer('RMi',  torch.zeros(num_features))\n",
        "            self.register_buffer('RVrr', torch.ones (num_features))\n",
        "            self.register_buffer('RVri', torch.zeros(num_features))\n",
        "            self.register_buffer('RVii', torch.ones (num_features))\n",
        "            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
        "        else:\n",
        "            self.register_parameter('RMr',                 None)\n",
        "            self.register_parameter('RMi',                 None)\n",
        "            self.register_parameter('RVrr',                None)\n",
        "            self.register_parameter('RVri',                None)\n",
        "            self.register_parameter('RVii',                None)\n",
        "            self.register_parameter('num_batches_tracked', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_running_stats(self):\n",
        "        if self.track_running_stats:\n",
        "            self.RMr.zero_()\n",
        "            self.RMi.zero_()\n",
        "            self.RVrr.fill_(1)\n",
        "            self.RVri.zero_()\n",
        "            self.RVii.fill_(1)\n",
        "            self.num_batches_tracked.zero_()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.reset_running_stats()\n",
        "        if self.affine:\n",
        "            self.Br.data.zero_()\n",
        "            self.Bi.data.zero_()\n",
        "            self.Wrr.data.fill_(1)\n",
        "            self.Wri.data.uniform_(-.9, +.9) # W will be positive-definite\n",
        "            self.Wii.data.fill_(1)\n",
        "\n",
        "    def _check_input_dim(self, xr, xi):\n",
        "        assert(xr.shape == xi.shape)\n",
        "        assert(xr.size(1) == self.num_features)\n",
        "\n",
        "    def forward(self, xr, xi):\n",
        "        self._check_input_dim(xr, xi)\n",
        "\n",
        "        exponential_average_factor = 0.0\n",
        "\n",
        "        if self.training and self.track_running_stats:\n",
        "            self.num_batches_tracked += 1\n",
        "            if self.momentum is None:  # use cumulative moving average\n",
        "                exponential_average_factor = 1.0 / self.num_batches_tracked.item()\n",
        "            else:  # use exponential moving average\n",
        "                exponential_average_factor = self.momentum\n",
        "\n",
        "        #\n",
        "        # NOTE: The precise meaning of the \"training flag\" is:\n",
        "        #       True:  Normalize using batch   statistics, update running statistics\n",
        "        #              if they are being collected.\n",
        "        #       False: Normalize using running statistics, ignore batch   statistics.\n",
        "        #\n",
        "        training = self.training or not self.track_running_stats\n",
        "        redux = [i for i in reversed(range(xr.dim())) if i!=1]\n",
        "        vdim  = [1] * xr.dim()\n",
        "        vdim[1] = xr.size(1)\n",
        "\n",
        "        #\n",
        "        # Mean M Computation and Centering\n",
        "        #\n",
        "        # Includes running mean update if training and running.\n",
        "        #\n",
        "        if training:\n",
        "            Mr, Mi = xr, xi\n",
        "            for d in redux:\n",
        "                Mr = Mr.mean(d, keepdim=True)\n",
        "                Mi = Mi.mean(d, keepdim=True)\n",
        "            if self.track_running_stats:\n",
        "                self.RMr.lerp_(Mr.squeeze(), exponential_average_factor)\n",
        "                self.RMi.lerp_(Mi.squeeze(), exponential_average_factor)\n",
        "        else:\n",
        "            Mr = self.RMr.view(vdim)\n",
        "            Mi = self.RMi.view(vdim)\n",
        "        xr, xi = xr-Mr, xi-Mi\n",
        "\n",
        "        #\n",
        "        # Variance Matrix V Computation\n",
        "        #\n",
        "        # Includes epsilon numerical stabilizer/Tikhonov regularizer.\n",
        "        # Includes running variance update if training and running.\n",
        "        #\n",
        "        if training:\n",
        "            Vrr = xr * xr\n",
        "            Vri = xr * xi\n",
        "            Vii = xi * xi\n",
        "            for d in redux:\n",
        "                Vrr = Vrr.mean(d, keepdim=True)\n",
        "                Vri = Vri.mean(d, keepdim=True)\n",
        "                Vii = Vii.mean(d, keepdim=True)\n",
        "            if self.track_running_stats:\n",
        "                self.RVrr.lerp_(Vrr.squeeze(), exponential_average_factor)\n",
        "                self.RVri.lerp_(Vri.squeeze(), exponential_average_factor)\n",
        "                self.RVii.lerp_(Vii.squeeze(), exponential_average_factor)\n",
        "        else:\n",
        "            Vrr = self.RVrr.view(vdim)\n",
        "            Vri = self.RVri.view(vdim)\n",
        "            Vii = self.RVii.view(vdim)\n",
        "        Vrr   = Vrr + self.eps\n",
        "        Vri   = Vri\n",
        "        Vii   = Vii + self.eps\n",
        "\n",
        "        #\n",
        "        # Matrix Inverse Square Root U = V^-0.5\n",
        "        #\n",
        "        # sqrt of a 2x2 matrix,\n",
        "        # - https://en.wikipedia.org/wiki/Square_root_of_a_2_by_2_matrix\n",
        "        tau   = Vrr + Vii\n",
        "        delta = torch.addcmul(Vrr * Vii, -1, Vri, Vri)\n",
        "        s     = delta.sqrt()\n",
        "        t     = (tau + 2*s).sqrt()\n",
        "\n",
        "        # matrix inverse, http://mathworld.wolfram.com/MatrixInverse.html\n",
        "        rst   = (s * t).reciprocal()\n",
        "        Urr   = (s + Vii) * rst\n",
        "        Uii   = (s + Vrr) * rst\n",
        "        Uri   = (  - Vri) * rst\n",
        "\n",
        "        #\n",
        "        # Optionally left-multiply U by affine weights W to produce combined\n",
        "        # weights Z, left-multiply the inputs by Z, then optionally bias them.\n",
        "        #\n",
        "        # y = Zx + B\n",
        "        # y = WUx + B\n",
        "        # y = [Wrr Wri][Urr Uri] [xr] + [Br]\n",
        "        #     [Wir Wii][Uir Uii] [xi]   [Bi]\n",
        "        #\n",
        "        if self.affine:\n",
        "            Wrr, Wri, Wii = self.Wrr.view(vdim), self.Wri.view(vdim), self.Wii.view(vdim)\n",
        "            Zrr = (Wrr * Urr) + (Wri * Uri)\n",
        "            Zri = (Wrr * Uri) + (Wri * Uii)\n",
        "            Zir = (Wri * Urr) + (Wii * Uri)\n",
        "            Zii = (Wri * Uri) + (Wii * Uii)\n",
        "        else:\n",
        "            Zrr, Zri, Zir, Zii = Urr, Uri, Uri, Uii\n",
        "\n",
        "        yr = (Zrr * xr) + (Zri * xi)\n",
        "        yi = (Zir * xr) + (Zii * xi)\n",
        "\n",
        "        if self.affine:\n",
        "            yr = yr + self.Br.view(vdim)\n",
        "            yi = yi + self.Bi.view(vdim)\n",
        "\n",
        "        return yr, yi\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return '{num_features}, eps={eps}, momentum={momentum}, affine={affine}, ' \\\n",
        "                'track_running_stats={track_running_stats}'.format(**self.__dict__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqTvrQa0aPRc",
        "colab_type": "text"
      },
      "source": [
        "### Unet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mDJRxHC7pSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NOTE: Use Complex Ops for DCUnet when implemented\n",
        "# Reference:\n",
        "#  > Progress: https://github.com/pytorch/pytorch/issues/755\n",
        "def pad2d_as(x1, x2):\n",
        "    # Pad x1 to have same size with x2\n",
        "    # inputs are NCHW\n",
        "    diffH = x2.size()[2] - x1.size()[2]\n",
        "    diffW = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "    return F.pad(x1, (0, diffW, 0, diffH)) # (L,R,T,B)\n",
        "\n",
        "def padded_cat(x1, x2, dim):\n",
        "    # NOTE: Use torch.cat with pad instead when merged\n",
        "    #  > https://github.com/pytorch/pytorch/pull/11494\n",
        "    x1 = pad2d_as(x1, x2)\n",
        "    x1 = torch.cat([x1, x2], dim=dim)\n",
        "    return x1\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, conv_cfg, leaky_slope):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv = ComplexConvWrapper(nn.Conv2d, *conv_cfg, bias=False)\n",
        "        self.bn = ComplexBatchNorm(conv_cfg[1])\n",
        "        self.act = CLeakyReLU(leaky_slope, inplace=True)\n",
        "\n",
        "    def forward(self, xr, xi):\n",
        "        xr, xi = self.act(*self.bn(*self.conv(xr, xi)))\n",
        "        return xr, xi\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dconv_cfg, leaky_slope):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dconv = ComplexConvWrapper(nn.ConvTranspose2d, *dconv_cfg, bias=False)\n",
        "        self.bn = ComplexBatchNorm(dconv_cfg[1])\n",
        "        self.act = CLeakyReLU(leaky_slope, inplace=True)\n",
        "\n",
        "    def forward(self, xr, xi, skip=None):\n",
        "        if skip is not None:\n",
        "            xr, xi = padded_cat(xr, skip[0], dim=1), padded_cat(xi, skip[1], dim=1)\n",
        "        xr, xi = self.act(*self.bn(*self.dconv(xr, xi)))\n",
        "        return xr, xi\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super(Unet, self).__init__()\n",
        "        self.encoders = nn.ModuleList()\n",
        "        for conv_cfg in cfg['encoders']:\n",
        "            self.encoders.append(Encoder(conv_cfg, cfg['leaky_slope']))\n",
        "\n",
        "        self.decoders = nn.ModuleList()\n",
        "        for dconv_cfg in cfg['decoders'][:-1]:\n",
        "            self.decoders.append(Decoder(dconv_cfg, cfg['leaky_slope']))\n",
        "\n",
        "        # Last decoder doesn't use BN & LeakyReLU. Use bias.\n",
        "        self.last_decoder = ComplexConvWrapper(nn.ConvTranspose2d,\n",
        "                *cfg['decoders'][-1], bias=True)\n",
        "\n",
        "        self.ratio_mask_type = cfg['ratio_mask']\n",
        "\n",
        "    def get_ratio_mask(self, outr, outi):\n",
        "        def inner_fn(r, i):\n",
        "            if self.ratio_mask_type == 'BDSS':\n",
        "                return torch.sigmoid(outr) * r, torch.sigmoid(outi) * i\n",
        "            else:\n",
        "                # Polar cordinate masks\n",
        "                # x1.4 slower\n",
        "                mag_mask = torch.sqrt(outr**2 + outi**2)\n",
        "                # M_phase = O/|O| for O = g(X)\n",
        "                # Same phase rotate(theta), for phase mask O/|O| and O.\n",
        "                phase_rotate = torch.atan2(outi, outr)\n",
        "\n",
        "                if self.ratio_mask_type == 'BDT':\n",
        "                    mag_mask = torch.tanh(mag_mask)\n",
        "                # else then UBD(Unbounded)\n",
        "\n",
        "                mag = mag_mask * torch.sqrt(r**2 + i**2)\n",
        "                phase = phase_rotate + torch.atan2(i, r)\n",
        "\n",
        "                # return real, imag\n",
        "                return mag * torch.cos(phase), mag * torch.sin(phase)\n",
        "\n",
        "        return inner_fn\n",
        "\n",
        "    def forward(self, xr, xi):\n",
        "        input_real, input_imag = xr, xi\n",
        "        skips = list()\n",
        "\n",
        "        for encoder in self.encoders:\n",
        "            xr, xi = encoder(xr, xi)\n",
        "            skips.append((xr, xi))\n",
        "\n",
        "        skip = skips.pop()\n",
        "        skip = None # First decoder input x is same as skip, drop skip.\n",
        "        for decoder in self.decoders:\n",
        "            xr, xi = decoder(xr, xi, skip)\n",
        "            skip = skips.pop()\n",
        "\n",
        "        xr, xi = padded_cat(xr, skip[0], dim=1), padded_cat(xi, skip[1], dim=1)\n",
        "        xr, xi = self.last_decoder(xr, xi)\n",
        "\n",
        "        xr, xi = pad2d_as(xr, input_real), pad2d_as(xi, input_imag)\n",
        "        ratio_mask_fn = self.get_ratio_mask(xr, xi)\n",
        "        return ratio_mask_fn(input_real, input_imag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0_woJfLaTDv",
        "colab_type": "text"
      },
      "source": [
        "### ISTFT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcSfB9X08ACw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ISTFT(torch.nn.Module):\n",
        "    def __init__(self, filter_length=1024, hop_length=512, window='hanning', center=True):\n",
        "        super(ISTFT, self).__init__()\n",
        "\n",
        "        self.filter_length = filter_length\n",
        "        self.hop_length = hop_length\n",
        "        self.center = center\n",
        "\n",
        "        win_cof = scipy.signal.get_window(window, filter_length)\n",
        "        self.inv_win = self.inverse_stft_window(win_cof, hop_length)\n",
        "\n",
        "        fourier_basis = np.fft.fft(np.eye(self.filter_length))\n",
        "        cutoff = int((self.filter_length / 2 + 1))\n",
        "        fourier_basis = np.vstack([np.real(fourier_basis[:cutoff, :]),\n",
        "                                   np.imag(fourier_basis[:cutoff, :])])\n",
        "        inverse_basis = torch.FloatTensor(self.inv_win * \\\n",
        "                np.linalg.pinv(fourier_basis).T[:, None, :])\n",
        "\n",
        "        self.register_buffer('inverse_basis', inverse_basis.float())\n",
        "\n",
        "    # Use equation 8 from Griffin, Lim.\n",
        "    # Paper: \"Signal Estimation from Modified Short-Time Fourier Transform\"\n",
        "    # Reference implementation: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/signal/spectral_ops.py\n",
        "    # librosa use equation 6 from paper: https://github.com/librosa/librosa/blob/0dcd53f462db124ed3f54edf2334f28738d2ecc6/librosa/core/spectrum.py#L302-L311\n",
        "    def inverse_stft_window(self, window, hop_length):\n",
        "        window_length = len(window)\n",
        "        denom = window ** 2\n",
        "        overlaps = -(-window_length // hop_length)  # Ceiling division.\n",
        "        denom = np.pad(denom, (0, overlaps * hop_length - window_length), 'constant')\n",
        "        denom = np.reshape(denom, (overlaps, hop_length)).sum(0)\n",
        "        denom = np.tile(denom, (overlaps, 1)).reshape(overlaps * hop_length)\n",
        "        return window / denom[:window_length]\n",
        "\n",
        "    def forward(self, real_part, imag_part, length=None):\n",
        "        if (real_part.dim() == 2):\n",
        "            real_part = real_part.unsqueeze(0)\n",
        "            imag_part = imag_part.unsqueeze(0)\n",
        "\n",
        "        recombined = torch.cat([real_part, imag_part], dim=1)\n",
        "\n",
        "        inverse_transform = F.conv_transpose1d(recombined,\n",
        "                                               self.inverse_basis,\n",
        "                                               stride=self.hop_length,\n",
        "                                               padding=0)\n",
        "\n",
        "        padded = int(self.filter_length // 2)\n",
        "        if length is None:\n",
        "            if self.center:\n",
        "                inverse_transform = inverse_transform[:, :, padded:-padded]\n",
        "        else:\n",
        "            if self.center:\n",
        "                inverse_transform = inverse_transform[:, :, padded:]\n",
        "            inverse_transform = inverse_transform[:, :, :length]\n",
        "\n",
        "        return inverse_transform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAYjpcBfab3-",
        "colab_type": "text"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEq9ibRv8OI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Params():\n",
        "    \"\"\"Class that loads hyperparameters from a json file.\n",
        "    Example:\n",
        "    ```\n",
        "    params = Params(json_path)\n",
        "    print(params.learning_rate)\n",
        "    params.learning_rate = 0.5  # change the value of learning_rate in params\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, json_path):\n",
        "        with open(json_path) as f:\n",
        "            params = json.load(f)\n",
        "            self.__dict__.update(params)\n",
        "\n",
        "    def save(self, json_path):\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(self.__dict__, f, indent=4)\n",
        "\n",
        "    def update(self, json_path):\n",
        "        \"\"\"Loads parameters from json file\"\"\"\n",
        "        with open(json_path) as f:\n",
        "            params = json.load(f)\n",
        "            self.__dict__.update(params)\n",
        "\n",
        "    @property\n",
        "    def dict(self):\n",
        "        \"\"\"Gives dict-like access to Params instance by `params.dict['learning_rate']\"\"\"\n",
        "        return self.__dict__\n",
        "\n",
        "\n",
        "class RunningAverage():\n",
        "    \"\"\"A simple class that maintains the running average of a quantity\n",
        "    Example:\n",
        "    ```\n",
        "    loss_avg = RunningAverage()\n",
        "    loss_avg.update(2)\n",
        "    loss_avg.update(4)\n",
        "    loss_avg() = 3\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.steps = 0\n",
        "        self.total = 0\n",
        "\n",
        "    def update(self, val):\n",
        "        self.total += val\n",
        "        self.steps += 1\n",
        "\n",
        "    def __call__(self):\n",
        "        return self.total / float(self.steps)\n",
        "\n",
        "\n",
        "def set_logger(log_path):\n",
        "    \"\"\"Set the logger to log info in terminal and file `log_path`.\n",
        "    In general, it is useful to have a logger so that every output to the terminal is saved\n",
        "    in a permanent file. Here we save it to `model_dir/train.log`.\n",
        "    Example:\n",
        "    ```\n",
        "    logging.info(\"Starting training...\")\n",
        "    ```\n",
        "    Args:\n",
        "        log_path: (string) where to log\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    if not logger.handlers:\n",
        "        # Logging to a file\n",
        "        file_handler = logging.FileHandler(log_path)\n",
        "        file_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
        "        logger.addHandler(file_handler)\n",
        "\n",
        "        # Logging to console\n",
        "        stream_handler = logging.StreamHandler()\n",
        "        stream_handler.setFormatter(logging.Formatter('%(message)s'))\n",
        "        logger.addHandler(stream_handler)\n",
        "\n",
        "\n",
        "def save_dict_to_json(d, json_path):\n",
        "    \"\"\"Saves dict of floats in json file\n",
        "    Args:\n",
        "        d: (dict) of float-castable values (np.float, int, float, etc.)\n",
        "        json_path: (string) path to json file\n",
        "    \"\"\"\n",
        "    with open(json_path, 'w') as f:\n",
        "        # We need to convert the values to float for json (it doesn't accept np.array, np.float, )\n",
        "        d = {k: float(v) for k, v in d.items()}\n",
        "        json.dump(d, f, indent=4)\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, checkpoint):\n",
        "    \"\"\"Saves model and training parameters at checkpoint + 'last.pth.tar'. If is_best==True, also saves\n",
        "    checkpoint + 'best.pth.tar'\n",
        "    Args:\n",
        "        state: (dict) contains model's state_dict, may contain other keys such as epoch, optimizer state_dict\n",
        "        is_best: (bool) True if it is the best model seen till now\n",
        "        checkpoint: (string) folder where parameters are to be saved\n",
        "    \"\"\"\n",
        "    filepath = os.path.join(checkpoint, 'last.pth.tar')\n",
        "    if not os.path.exists(checkpoint):\n",
        "        print(\"Checkpoint Directory does not exist! Making directory {}\".format(checkpoint))\n",
        "        os.mkdir(checkpoint)\n",
        "    else:\n",
        "        print(\"Checkpoint Directory exists! \")\n",
        "    torch.save(state, filepath)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filepath, os.path.join(checkpoint, 'best.pth.tar'))\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer=None):\n",
        "    \"\"\"Loads model parameters (state_dict) from file_path. If optimizer is provided, loads state_dict of\n",
        "    optimizer assuming it is present in checkpoint.\n",
        "    Args:\n",
        "        checkpoint: (string) filename which needs to be loaded\n",
        "        model: (torch.nn.Module) model for which the parameters are loaded\n",
        "        optimizer: (torch.optim) optional: resume optimizer from checkpoint\n",
        "    \"\"\"\n",
        "    if not os.path.exists(checkpoint):\n",
        "        raise (\"File doesn't exist {}\".format(checkpoint))\n",
        "    checkpoint = torch.load(checkpoint) \n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    best_pesq = checkpoint['best_pesq']\n",
        "    best_stoi = checkpoint['best_stoi']\n",
        "\n",
        "    if optimizer:\n",
        "        optimizer.load_state_dict(checkpoint['optim_dict'])\n",
        "\n",
        "    return best_pesq, best_stoi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WhcOlmMakIZ",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQrWLPvr8maC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fft, hop_length = 400, 160\n",
        "window = torch.hann_window(n_fft).cuda()\n",
        "# STFT\n",
        "stft = lambda x: torch.stft(x, n_fft, hop_length, window=window)\n",
        "# ISTFT\n",
        "istft = ISTFT(n_fft, hop_length, window='hanning').cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFC8VEok9vmy",
        "colab_type": "code",
        "outputId": "ac7ea61b-bec1-489d-ea78-0089d85c61b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# hyper-parameters\n",
        "max_epoch = 60\n",
        "learning_rate = 0.001\n",
        "batch_size = 30\n",
        "device = 'cuda'\n",
        "\n",
        "base_param = {\"seed\" : 2017, \"save_path\" : \"/deep/group/awni/speech_models/test\",\n",
        "    \"data\" : {\n",
        "        \"train_set\" : \"/deep/group/speech/datasets/LibriSpeech/train-toy.json\",\n",
        "        \"dev_set\" : \"/deep/group/speech/datasets/LibriSpeech/dev-toy.json\"\n",
        "    },\n",
        "\n",
        "    \"optimizer\" : {\n",
        "        \"batch_size\" : 8,\n",
        "        \"epochs\" : 1000,\n",
        "        \"learning_rate\" : 1e-3,\n",
        "        \"momentum\" : 0.0\n",
        "    },\n",
        "\n",
        "    \"model\" : {\n",
        "        \"leaky_slope\" : 0.1,\n",
        "        \"ratio_mask\" : \"BDT\",\n",
        "        \"encoders\" : [\n",
        "            [1, 32, [7, 5], [2, 2], [3, 2]],\n",
        "            [32, 32, [7, 5], [2, 1], [3, 2]],\n",
        "            [32, 64, [7, 5], [2, 2], [3, 2]],\n",
        "            [64, 64, [5, 3], [2, 1], [2, 1]],\n",
        "            [64, 64, [5, 3], [2, 2], [2, 1]],\n",
        "            [64, 64, [5, 3], [2, 1], [2, 1]],\n",
        "            [64, 64, [5, 3], [2, 2], [2, 1]],\n",
        "            [64, 64, [5, 3], [2, 1], [2, 1]]\n",
        "        ],\n",
        "        \"decoders\" : [\n",
        "            [64, 64, [5, 3], [2, 1], [2, 1]],\n",
        "            [128, 64, [5, 3], [2, 2], [2, 1]],\n",
        "            [128, 64, [5, 3], [2, 1], [2, 1]],\n",
        "            [128, 64, [5, 3], [2, 2], [2, 1]],\n",
        "            [128, 64, [5, 3], [2, 1], [2, 1]],\n",
        "            [128, 32, [7, 5], [2, 2], [3, 2]],\n",
        "            [64, 32, [7, 5], [2, 1], [3, 2]],\n",
        "            [64, 1, [7, 5], [2, 2], [3, 2]]\n",
        "        ],\n",
        "        \"__coder_keys\" : [\n",
        "            \"in_channels\", \"out_channels\", \"kernel_size\", \"stride\", \"padding\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# initialize tensorboard for visualization\n",
        "tbc = TensorBoardColab()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "http://a4a882f9.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yHCzGWW-PuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data loader\n",
        "train_dataset = AudioDataset(data_type='train')\n",
        "test_dataset = AudioDataset(data_type='val')\n",
        "train_data_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, collate_fn=train_dataset.collate, shuffle=True, num_workers=4)\n",
        "test_data_loader = DataLoader(dataset=test_dataset, batch_size=args.batch_size, collate_fn=test_dataset.collate, shuffle=False, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uem3q_qy-RIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model set\n",
        "net = Unet(base_param['model']).cuda()\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXBdezS4_rPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check point load\n",
        "\n",
        "ckpt_dir = os.path.join(gdrive_root, 'checkpoints')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "  os.makedirs(ckpt_dir)\n",
        "\n",
        "best_pesq = 0.\n",
        "best_stoi = 0.\n",
        "ckpt_path = os.path.join(ckpt_dir, 'SEckpt.pt')\n",
        "if os.path.exists(ckpt_path):\n",
        "  ckpt = torch.load(ckpt_path)\n",
        "  try:\n",
        "    net.load_state_dict(ckpt['model'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    best_pesq = ckpt['best_pesq']\n",
        "  except RuntimeError as e:\n",
        "      print('wrong checkpoint')\n",
        "  else:    \n",
        "    print('checkpoint is loaded !')\n",
        "    print('current best pesq : %.4f' % best_pesq)\n",
        "    print('current best stoi : %.4f' % best_stoi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OZQYGUaCxX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train & test\n",
        "iteration = 0\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "      train_bar = tqdm(train_data_loader)\n",
        "        for input in train_bar:\n",
        "            iteration += 1\n",
        "            #load data\n",
        "            train_mixed, train_clean, seq_len = map(lambda x: x.cuda(), input)\n",
        "            mixed = stft(train_mixed).unsqueeze(dim=1)\n",
        "            real, imag = mixed[..., 0], mixed[..., 1]\n",
        "\n",
        "            #feed data\n",
        "            out_real, out_imag = net(real, imag)\n",
        "            out_real, out_imag = torch.squeeze(out_real, 1), torch.squeeze(out_imag, 1)\n",
        "            out_audio = istft(out_real, out_imag, train_mixed.size(1))\n",
        "            out_audio = torch.squeeze(out_audio, dim=1)\n",
        "            for i, l in enumerate(seq_len):\n",
        "                out_audio[i, l:] = 0\n",
        "            librosa.output.write_wav('mixed.wav', train_mixed[0].cpu().data.numpy()[:seq_len[0].cpu().data.numpy()], 16000)\n",
        "            librosa.output.write_wav('clean.wav', train_clean[0].cpu().data.numpy()[:seq_len[0].cpu().data.numpy()], 16000)\n",
        "            librosa.output.write_wav('out.wav', out_audio[0].cpu().data.numpy()[:seq_len[0].cpu().data.numpy()], 16000)\n",
        "\n",
        "            #calculate LOSS\n",
        "            #loss =  wSDRLoss(train_mixed, train_clean, out_audio)\n",
        "            loss = torch.nn.MSELoss(out_audio, train_clean)\n",
        "            #gradient optimizer\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #backpropagate LOSS\n",
        "            loss.backward()\n",
        "\n",
        "            #update weight\n",
        "            optimizer.step()\n",
        "\n",
        "            #calculate accuracy\n",
        "            PESQ = pesq('clean.wav', 'out.wav', 16000)\n",
        "            STOI = stoi('clean.wav', 'out.wav', 16000)\n",
        "\n",
        "            #flot tensorboard\n",
        "            if iteration % \"num\" == 0:\n",
        "                summary.add_scalar('Train Loss', loss.item(), iteration)\n",
        "                tbc.save_value('Loss', 'Train Loss', iteration ,loss.item())\n",
        "                print('[epoch: {}, iteration: {}] train loss : {:.4f} PESQ : {:.4f} STOI : {:.4f}'.format(epoch, iteration, loss, PESQ, STOI))\n",
        "\n",
        "            #test phase\n",
        "            n = 0\n",
        "            test_loss = 0\n",
        "            test_acc = 0\n",
        "            test_bar = tqdm(test_train_loader)\n",
        "            for input in test_bar:\n",
        "                test_mixed, test_clean, test_seq_len = map(lambda x: x.cuda(), input)\n",
        "                logits_real, logits_imag = net(input)\n",
        "                logits_real, logits_imag = torch.squeeze(logits_real, 1), torch.squeeze(out_imag, 1)\n",
        "                logits_audio = istft(logits_real, logits_imag, test_mixed.size(1))\n",
        "                logits_audio = torch.squeeze(logits_audio, dim=1)\n",
        "                for i, l in enumerate(test_seq_len):\n",
        "                logits_audio[i, l:] = 0\n",
        "                librosa.output.write_wav('test_clean.wav', test_clean[0].cpu().data.numpy()[:seq_len[0].cpu().data.numpy()], 16000)\n",
        "                librosa.output.write_wav('test_out.wav', logits_audio[0].cpu().data.numpy()[:seq_len[0].cpu().data.numpy()], 16000)\n",
        "\n",
        "                #test loss\n",
        "                #test_loss = wSDRLoss(test_mixed, test_clean, logits_audio)\n",
        "                test_loss += torch.nn.MSELoss(logits_audio, test_clean, reduction = 'sum').item()\n",
        "                #test accuracy\n",
        "                test_pesq = pesq('test_clean.wav', 'test_out.wav', 16000)\n",
        "                test_stoi = stoi('test_clean.wav', 'test_out.wav', 16000)\n",
        "\n",
        "            tbc.save_value('Loss', 'Test_loss', iteration, test_loss)\n",
        "            print('[epoch: {}, iteration: {}] test loss : {:.4f} PESQ : {:.4f} STOI : {:.4f}'.format(epoch, iteration, test_loss, test_pesq, test_stoi))\n",
        "            tbc.flush_line('Train loss')\n",
        "            tbc.flush_line('Test loss')\n",
        "            \n",
        "            if test_pesq > best_pesq or test_stoi > best_stoi:\n",
        "                best_pesq = test_pesq\n",
        "                best_stoi = test_stoi\n",
        "                # Note: optimizer also has states ! don't forget to save them as well.\n",
        "                ckpt = {'model':net.state_dict(),\n",
        "                        'optimizer':optimizer.state_dict(),\n",
        "                        'best_pesq':best_pesq}\n",
        "                torch.save(ckpt, ckpt_path)\n",
        "                print('checkpoint is saved !')\n",
        "\n",
        "tbc.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}